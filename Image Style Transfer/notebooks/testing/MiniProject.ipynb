{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dbf0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c54061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the same model architecture from your training\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(reflection_padding),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvLayer(channels, channels, 3, 1),\n",
    "            ConvLayer(channels, channels, 3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvLayer(3, 32, 9, 1),\n",
    "            ConvLayer(32, 64, 3, 2),\n",
    "            ConvLayer(64, 128, 3, 2),\n",
    "        )\n",
    "        self.residuals = nn.Sequential(\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ConvLayer(128, 64, 3, 1),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ConvLayer(64, 32, 3, 1),\n",
    "            ConvLayer(32, 3, 9, 1),\n",
    "            nn.Tanh(),  # [-1, 1] output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.residuals(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "556bd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_dir, output_dir, model_path, image_size=256):\n",
    "    \"\"\"Process all images in a directory\"\"\"\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the model\n",
    "    print(\"Loading model...\")\n",
    "    transformer = TransformerNet().to(device)\n",
    "    transformer.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    transformer.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x * 2 - 1)  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Find all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\"]\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(input_dir, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(input_dir, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No image files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "        try:\n",
    "            # Get filename without directory\n",
    "            filename = os.path.basename(img_path)\n",
    "            output_path = os.path.join(output_dir, f\"styled_{filename}\")\n",
    "            \n",
    "            # Load and preprocess the image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Perform style transfer\n",
    "            with torch.no_grad():\n",
    "                output_tensor = transformer(input_tensor)\n",
    "            \n",
    "            # Save the output image\n",
    "            output = output_tensor[0].cpu()\n",
    "            output = output * 0.5 + 0.5  # Denormalize from [-1,1] to [0,1]\n",
    "            output = output.clamp(0, 1)\n",
    "            output_image = transforms.ToPILImage()(output)\n",
    "            output_image.save(output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    print(f\"Processing complete! Stylized images saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17d2b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Found 2 images to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Stylized images saved to /Users/aya/Desktop/deep learning/miniproject/newimages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check if running in Jupyter\n",
    "    try:\n",
    "        get_ipython()\n",
    "        # Running in Jupyter, so define the variables directly\n",
    "        input_dir = \"/Users/aya/Desktop/deep learning/miniproject/images\"\n",
    "        output_dir = \"/Users/aya/Desktop/deep learning/miniproject/newimages\"\n",
    "        model_path = \"/Users/aya/Desktop/deep learning/miniproject/fast_style_transfer_model.pth\"\n",
    "        image_size = 256\n",
    "    except:\n",
    "        # Not in Jupyter, use argparse\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(description='Apply style transfer to a directory of images')\n",
    "        parser.add_argument('--input_dir', type=str, required=True, help='Directory containing input images')\n",
    "        parser.add_argument('--output_dir', type=str, required=True, help='Directory to save stylized images')\n",
    "        parser.add_argument('--model_path', type=str, default='fast_style_transfer_model.pth', help='Path to the trained model')\n",
    "        parser.add_argument('--image_size', type=int, default=256, help='Size to resize images to')\n",
    "        args = parser.parse_args()\n",
    "        input_dir = args.input_dir\n",
    "        output_dir = args.output_dir\n",
    "        model_path = args.model_path\n",
    "        image_size = args.image_size\n",
    "    \n",
    "    # Call the function with the defined variables\n",
    "    process_directory(input_dir, output_dir, model_path, image_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
